###################################
# CS B551 Fall 2015, Assignment #5
#
# Manish Kumar : Kumar20@iu.edu
# Thij Bens :    thijbens@indiana.edu
# (Based on skeleton code by D. Crandall)
#
#
####******************************************** Report ******************************************
# 1. Method: Instead of storing the probability, we are storing just the counts of all parts of speech
#    occurring individually, occurring in a group of two as well as for words, the number of times it occur
#    as  different parts of speech in the training data. We also assigned some minor value to the words or 
#	 pair of parts of speech we have not seen so far. Subsequently, as per the inference algorithm,we compute the 
#    probability of sequence of parts of speech for words given as  observed variables. For Naive bayes, 
#    probability of parts of speech given word would be independent of other words and parts of speech assigned 
#    to them. So, we just select the parts of speech that comes maximum number of times in that particular word 
#    dictionary. For Sampling, we started with the naive bayes distribution and then continued to select variable 
#    and compute the probability for each pos given the rest of the distribuition and then generated a random number
#    and selected the pos as per the region in which random number falls. We threw the first 100 samples, so, as it 
#    would converge to stationary distribuition and then collected the samples. For Max Marginal, we collected 1000 
#	 sample generated by MCMC method and by majority voting decided the pos for each word. For MAP, by implementing
#	 dynamic programing, we computed the sequence of pos with maximum probability. The sequence with maximum
#	 probability would be the sequence of pos for the given word sequence. Our best algorithm is viterbi algorithm.
#    Though on test data provided, Max marginal provides slightly better result than our best algorithm. But we
#    believe that on overall test scenario, it would beat the Max Marginal.
#
# 2. Result: This program runs on test data of 2000 sentences with 29442 words and achieves following performance:
#					algorithm 					Words Correct 						Sentence Correct				
#				0:  Ground Truth: 					100%								100%
#				1:	Naive Bayes:					93.96%								47.45%  
#				2:  sampler                         93.13%								44.50%
#				3:  Max Marginal  					94.70% 								53.05%
#				4.  MAP 							94.58%   							52.10%
#				5.  Best 							94.58%								52.10%
#
# 3. Discussion: 
#				Problems: We were implementing trigram hidden markov model for our best algorithm, but we did not succeed in it.
#						  It was taking too long to execute and did not give the desired result. We are hoping to implement it in
#						  in future. We also had to deal with the unseen words or unseen binary sequence of pos.
#
#				Assumption: For each word, we are assigning very small probability to each pos irrespective of whether the word as 
#							that pos exist or not in our training data.
#							If the word does not exist in our training data, we are assigning small probability to unseen words as well.
#				
#				Simplification: In our best algorithm, we are just calling the viterbi algorithm implemented in the class.
#							 
#####
import random
import math
from collections import defaultdict
from itertools import izip

# We've set up a suggested code structure, but feel free to change it. Just
# make sure your code still works with the label.py and pos_scorer.py code
# that we've supplied.
#ot

											# tag_first keeps the count of



class Solver:
	def __init__(self):
		self.tag_binary = defaultdict(float)
		self.tag_all = defaultdict(float)
		self.tag_all_words = defaultdict(dict)
		self.tag_trigram = defaultdict(float)
		self.wordCount=0
		self.pos = ['adv', 'noun', 'adp', 'pron', 'det', 'num', '.', 'prt', 'verb', 'x', 'conj', 'adj']

	# Calculate the log of the posterior probability of a given sentence
	#  with a given part-of-speech labeling
	def posterior(self, sentence, label):
		tag_binary = self.tag_binary
		tag_all = self.tag_all
		tag_all_words = self.tag_all_words
		start_count = 0;
		binary_count = 0
		for key in tag_binary.keys():
			if key[0] == '*':
				start_count += tag_binary[key]
			elif key[1] != '*':
				binary_count += tag_binary[key]
		count = 0
		log_sum = 0
		for (pos,word) in izip(label,sentence):
			if count < len(label):
				if count == 0:
					log_sum += math.log((tag_binary[('*',pos)]/float(start_count)))
				else:
					log_sum += math.log((tag_binary[(label[count-1],pos)])/(float(tag_all[label[count-1]])))
			log_sum += math.log(tag_all_words[word][pos]/(tag_all[pos]))
			count += 1
		return log_sum
#
    # Do the training!
    #
	def train(self, data):
		tag_binary = self.tag_binary
		tag_all = self.tag_all
		tag_all_words = self.tag_all_words

		for mytuple in data:
			word_tuple = mytuple[0]
			tag_tuple = mytuple[1]
			tag_binary[('*',tag_tuple[0])] += 1.0
			length = len(tag_tuple)
			for i in range(length):
				tag_all[tag_tuple[i]] += 1.0
				if tag_tuple[i] not in tag_all_words[word_tuple[i]].keys():
					for p in self.pos:
						tag_all_words[word_tuple[i]][p] = 0.01
				tag_all_words[word_tuple[i]][tag_tuple[i]] += 1.0
				if i < length-1:
					tag_binary[(tag_tuple[i],tag_tuple[i+1])] += 1.0
				elif i == length-1:
					tag_binary[(tag_tuple[i],'*')] += 1.0
		for key in tag_all:
			self.wordCount += tag_all[key]

                # print key, tag_binary[key]

    # Functions for each algorithm.
	def naive(self, sentence):
		tag_binary = self.tag_binary
		tag_all = self.tag_all
		tag_all_words = self.tag_all_words
		pos_list = []
		for word in sentence:
			if len(tag_all_words[word]) == 0:
		            tag_all_words[word] = tag_all
		        pos = self.max_pos(tag_all_words[word])[0]
			pos_list.append(pos)
		return [[pos_list] , [] ]


	def mcmc(self, sentence, sample_count):
		tag_binary = self.tag_binary
		tag_all = self.tag_all
		tag_all_words = self.tag_all_words

        # print tag_all_words['can']
		ret = []
        #generate initial sample based on P(S|W)
		samp = {}
		count = 0
		for word in sentence:
			if len(tag_all_words[word]) == 0:
				tag_all_words[word] = tag_all
			samp[count] = self.max_pos(tag_all_words[word])[0]
			count += 1

		burn_in = 100
       	#sample sample_count times + some burn in
		for i in range(burn_in+sample_count):
			new_samp = samp
			new_prob = defaultdict(list)

            #calculate probabilities of each part of speech
			count = 0
			for w in sentence:
				obs_count = sum(tag_all_words[w][p1] for p1 in self.pos)
				for p in self.pos:
                    #P(S_2 | S_1=noun) P(S3=noun | S2) P(W_2=obs | S2)
					prob = (tag_binary[(samp[count-1] if count-1 >=0 else '*',p)]/tag_all[samp[count]]) * (tag_binary[(p,samp[count+1] if count+1 < len(sentence) else '*')]/tag_all[p]) * (tag_all_words[w][p]/obs_count)
					new_prob[count].append((p,prob))
				count += 1

            # generate new sample based on these probabilities
			count = 0
			for w in new_prob:
                #total of small probabilities
				pr_sum = 0
				for tu in new_prob[w]:
					pr_sum += tu[1]

                #normalize probabilities, should sum to 1
				new_dict = defaultdict(float)
				for tu in new_prob[w]:
					new_dict[tu[0]] = tu[1]/pr_sum

                #sample from normalized probability spac
				r = random.random()
				c_sum = 0
				new_tag = ""
				for nd in new_dict:
					c_sum += new_dict[nd]
					if r <= c_sum:
						new_tag = nd
						break

				new_samp[count] = new_tag
				count += 1

            #replace the existing sample
			samp = new_samp
			if i >= burn_in:
				samp_list = []
				for key in sorted(samp.keys()):
					samp_list += [samp[key]]
				ret.append(samp_list)


		return [ret, []]

	def best(self, sentence):
		return self.viterbi(sentence)

	def max_marginal(self, sentence):
		count_majority = defaultdict(dict)
		confidence = []
		tag_list = []
		for word in sentence:
			for pos in self.pos:
				count_majority[word][pos] = 0.0
		[pos_list,[]] = self.mcmc(sentence,1000)
		for each_list in pos_list:
			for pos,word in izip(each_list,sentence):
				count_majority[word][pos] += 1
		for key in count_majority.keys():
			totalCount = 0
			for pos_tag in count_majority[key]:
				totalCount += count_majority[key][pos_tag]
			count_majority[key]["count"]= totalCount
		for word in sentence:
			major = 0
			tag = ""
			for pos in count_majority[word]:
				 if pos != "count" and count_majority[word][pos] > major:
					major = count_majority[word][pos]
					tag = pos
			tag_list.append(tag)
			confidence.append(major/float(count_majority[word]["count"]))

		return  [[tag_list], [confidence] ]

	def viterbi(self, sentence):
		tag_binary = self.tag_binary
		tag_all = self.tag_all
		tag_all_words = self.tag_all_words
		pos = self.pos
		table = [{}]
		path = {}

		#base case
		obs_count = sum(tag_all_words[sentence[0]][p1] for p1 in self.pos)
		for p in pos:
			table[0][p] = (tag_binary[('*',p)]/tag_all[p]) * (tag_all_words[sentence[0]][p]/obs_count)
			path[p] = [p]

		for i in range(1, len(sentence)):
			table.append({})
			newpath = {}
			obs_count = sum(tag_all_words[sentence[i]][p1] for p1 in self.pos)
			for p in pos:
				(prob, state) = max((table[i-1][p1] * (tag_binary[(p1,p)]/tag_all[p1]) * (tag_all_words[sentence[i]][p]/obs_count), p1) for p1 in pos)
				table[i][p] = prob
				newpath[p] = path[state] + [p]

			path = newpath

		(prob, state) = max((table[len(sentence)-1][p],p) for p in pos)

		return [ [path[state]], [] ]

    # This solve() method is called by label.py, so you should keep the listerface the
    #  same, but you can change the code itself.
    # It's supposed to return a list with two elements:
    #
    #  - The first element is a list of part-of-speech labelings of the sentence.
    #    Each of these is a list, one part of speech per word of the sentence.
    #    Most algorithms only return a single labeling per sentence, except for the
    #    mcmc sampler which is supposed to return 5.
    #
    #  - The second element is a list of probabilities, one per word. This is
    #    only needed for max_marginal() and is the marginal probabilities for each word.
    #
	def solve(self, algo, sentence):
		if algo == "Naive":
			return self.naive(sentence)
		elif algo == "Sampler":
			return self.mcmc(sentence, 5)
		elif algo == "Max marginal":
			return self.max_marginal(sentence)
		elif algo == "MAP":
			return self.viterbi(sentence)
		elif algo == "Best":
			return self.best(sentence)
		else:
			print "Unknown algo!"

	def max_pos(self, d):
		m = 0
		b = ""
		for p in d:
			if (d[p] + (self.tag_all[p]/self.wordCount))  > m:
				m = max(m,d[p]) +(self.tag_all[p]/self.wordCount)
				b = p
		return (b, m)
